# 反爬

    requests 解码方式的设置 requests.encoding = '需要转码的类型'
### 1 
    雪球用 添加表头 headers 一般的都需要加啦没毛病 | 包含cookic 
###  2
     12306 添加 ssl._create_default_https_context = ssl._create_unverified_context
### 3 
    用代理IPs
### 4 
     有道翻译  （MD5加密处理） 时间撮+s随机数 
### 5 
    网站证书验证 request使用ssi  requserts  ,(verify=False )

### 6  煎蛋图片
    1.  找到 js 分析JS，
    2.   base64 和md5 加密
    3. 煎蛋  反向找出js加密函数 用exejs包 运行js模块 运行加密  
    4. 分析出图地址  下载 就OK啦
### 7 拉钩 josn 数据
    反扒 设置啦 headers 里 传值data的长度 len(data)

# js正向解析
    pip instill selenium
    
    下载 http://phantomjs.org/ 
     phantomjs的工具
# 503 错误 request 
    headers 的添加
    res = request.Reques(url=url,headers=headers)
    request.urlopen(res)
    


# os 模块 在爬虫的使用  创建文件夹

    root_dir = '文件夹名字'
    # 判断文件夹 是否存在  如果存在 不创建   不存在则创建新的文件
    if not os.path.exists(文件夹名字)
        os.mkdir(文件夹名字)
# request  使用技巧
    1. URL 和 重网页上获取 新的url连接   进行进一步的 拼接  如下
        href = /zufang/41319163.html
        url = https://bj.5i5j.com/zufang/41319163.html

        new_url = request.urljoin(url,href)

# 在学习Python爬虫过程中，主要用两个模块：
## selecet   数据的出的是列表  
##find  
##findall  的使用

    例子：
        from bs4 import BeautifulSoup
        import requests
        import lxml
        url = 'https://bj.lianjia.com/zufang/'
        response = requests.get(url)
        html = response.text
        # print(html)
        res = BeautifulSoup(html,'lxml')
        # 得到 id 是house-lst 的ul
        ul_one = res.find('ul',id="house-lst")
        # 得到ul 里全部的 li   遍历
        li_list = ul_one.find_all('li')
        for li in li_list:
            # 连接   数据的出的是列表  取0
            a = li.select('h2 > a')[0]
            href = a['href']
            print(href)
-------
    import requests
    from bs4 import BeautifulSoup

    requests用于将链接转化成html语言，而BeautifulSoup则用于查找需要的内容。
    最开始一般写成如下格式：
    res = requests.get('https://www.qiushibaike.com/hot/#')  #以糗事百科为例
    soup = BeautifulSoup(res,'html.parser')
    通过采用soup.select()方法，可以得到所需的内容。 
    其中关键点在于，对于所需内容的精准定位，通过（）内的语句来实现：class 
    对于html内的内容，可以通过class来进行定位，一般形式为：
    soup.selecet('.class')

##  基本用法
    html.find_all('p','div')  获取div 里的所有p 和 div 标签
##  css 选择器用法
    #p下 子孙节点
    html.select('p>.title') # 选择 p标签下的 属性类名为title 标签   #title id名  
    #p下 子节点  
    html.select('p.title') # 选择 p标签下的 属性类名为title 标签   #title id名  
       
    html = BeautifulSoup(html,'lxml')
    
    #print(html.select('p.title'))
    # print(html.select('p .b1'))    # p下的子孙节点查找
    # print(html.select('p > .b1'))  # p下的子节点查找
    
    # id选择
    # print(html.select('p#p1'))
    # print(html.select('p b'))
    
    # 或者的关系
    # print(html.select('p.story1,p.story2,.story3'))
    
    # 根据属性筛选
    # print(html.select('p[class="story1"]'))
    
    # 选出类是以s开头的p标签
    # print(html.select('p[class^="s"]'))
    
    # 选出类是以结束的p标签
    # print(html.select('p[class$="s"]'))
    
    # 模糊查询
    # print(html.select('p[class*="k"]'))

    # 每一个li标签下面的ul标签, 他的class=event-meta, 这个ul标签下的第二个li标签
    address = li_tag.select('ul.event-meta > li:nth-of-type(2)')[0].text.replace('\n','').strip()
## find_all 筛选 
    info = html.find_all('p',attrs={'class':'title','id':'p1'})
    # 筛选   p  标签里的所有内容   通过get.text()  也个简写i.text
    info = html.find_all('p',attrs={'class':'story'})
    for i in info:
        print(i.get_text())


#  day 4  爬取ip 存储JOSN 数据的  
1.      enumerate 函数的用法
        host = ['192.1.1.1',193.2.2.2]
        enumerate(host)
        返回义个对象
        
        index,value = enumerate(host)
        返回:（0，'192.1.1.1'）  元组
        for index,value in enumerate(host)
            print（）
        返回:（0，'192.1.1.1'）  元组
            （1，'192.1.1.1'）
        拿到列表索引和值
             拿列表的长度判断  是否是最后一行
        
        enmerata(host)
# day 5 
## jsonpath  解决json多层嵌套

1. ​

    jsonpath.jsonpath()


## lxml     返回的是列表    标签语言
## xpath 用法  两个字符串的拼接  parse.urljoin('字符串1','字符串1')
    from lxml import etree
    //  所有的  不管在那
    /   根目录
    text() 获得里面的内容
    |   或者
    @   获取 属性名字
    []  过滤器   索引重1 开始
    [position()<3]     设置索引条件
    标签名[@属性名]  把title 所有带lang属性的 都找出来
    
    精准配备：
    标签名[@属性名=名字] 把title 所有带lang属性的=sum 都找出来
    
    支持 标签里的 比如钱数 大于 多多少钱的
    [标签名字>35]    把price 里钱数大于 35的 都取出来
     
    
    路径 表达式
    /节点/*   选取根节点下的所有元素
    //*  选取文档中的所有元素
    //title[@]  选取 title 里 有带属性的 全选
    res = html.xpath('//bookstore/*')
    主要提取html 标签里的内容


## replace  python函数

        replace('a','b')  把a换成b
        replace('空格','空')  把空格换成空


# join 连接 ''.join(duty_list)  
        duty = ''.join(duty_list) #  以空格分隔   同时转化str    
    

## isspace() 检测字符中所有的字符是否全部是空白字符 返回bool值
## splitlines()  将字符串按换号位置进行切割
## lstrip()    lstrip()   去掉左右的指定字符 默认空格
    1.strip()：把头和尾的空格去掉
    2.lstrip()：把左边的空格去掉
    3.rstrip()：把右边的空格去掉
    4.replace('c1','c2')：把字符串里的c1替换成c2。故可以用replace(' ','')来去掉字符串里的所有空格
    切片
    5.split()：通过指定分隔符对字符串进行切片，如果参数num 有指定值，则仅分隔 num 个子字符串
# 去除文本 标签 from w3lib.html import remove_tags
    html = remove_tags(html).replace('\n','')
    print(html)
    

# 进程池 队列问题    Queue() 之间无法通信    建立通信加.Manager()
    1.  .Manager()   问题
        import multiprocessing
        # 实例化 加.Manager() 可以在 asge 传多个参数 
        P = multiprocessing.Manager().Queue()
        q_qool.apply_async(Verification,(任意一个参数,任意一个队列))

    2. 
        import multiprocessing
        # 实例化 不加.Manager() 可以在 asge 传多个参数 
        P = multiprocessing.Queue() # 
        q_qool.apply_async(Verification,(任意一个参数,))
    
# 池 Queue()  
    op = Pool() # 实例化
    q = Manager().Queue() # 使用Manager中的Queue来初始化
    op.apply() # 同步
    op.apply_async()# 异步
# 进程  池   线程 池
    1.
        import threadpool
        pool = threadpool.ThreadPool(10) 
        pool = threadpool.makeRequests(函数, 列表)
        pool.wait()

## 常用函数  池 Queue()  

    1. Queue().qsize() 返回党项队列包含的的数量
    2.Queue().full   表示当前队列是否积已经满啦 True 满 False为满
    3.Queue().empty()# 判断队列是否为空 Ture 为空
    
    
    4.Queue().put(消息,block=Ture,timeout=None)
        blick (默认Ture)一直阻塞  直到put进去为止
        tomeout(默认None) 一直等 如果timeout 否则等到N面 然后强制put
    5. Queue().get(block=Ture,timeout=None)
    
    
    6. Queue().put_nowit()  相当于 Queue().put(消息,False)
    7.  q1.get_nowaif()
    get() 和 
    if not Queue().empty(): # 判断队列是否为空
        for i in range(q1.size())
            q1.get_nowaif()

# 进程 

    import multiprocessing # 导入
    # 定义进程函数
    def foo():
        print('进程')
    if __name__ == '__main__':
        #  创建 进程 需要线定一个函数
        # target ： 进程函数对象
        # args ：进程函数参数 
        m=multiprocessing.Process(func=foo,args=())
        m.start()
        m.join() # 进程执行中会阻塞主进程
# 线程  注意线程的 开启 和 关闭   整不好 会变成单线程  

        import threading #导入线程
        import time
        def foo(i):
            print('%d号开始'% i)
            time.sleep(5)
            print('%d号结束' % i)
    
        t_list = []
        for i in range(1,10):
            t1 = threading.Thread(target=foo,args=(i,))
            t1.start()
            print(t1.is_alive())  # 查看 线程状态
            t_list.append(t1)
        # 所有线程执行完毕 运行 列表结束线程
        for t in t_list:
            t.join()
            print(t.is_alive()) # 查看 线程状态
            print('结束')
# 腾讯线程
        import threading
        import requests
        from bs4 import BeautifulSoup
        class MyThread(threading.Thread):
            # 继承 threading
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36"
            }
            baseurl = 'https://hr.tencent.com/position.php?lid=2156&tid=&start=%d#a'
            def __init__(self,i):
                super(MyThread,self).__init__()
                self.i = i
    
            # 线程启动 时候被调用 getPage
            def run(self):
                self.getPage(self.i)
    
            def getPage(self,i):
                fullurl = self.baseurl % i
                print(fullurl)
                resopense = requests.get(url=fullurl, headers=self.headers)
                html = resopense.text
                html = BeautifulSoup(html, 'lxml')
                tr_list = html.select('table.tablelist tr')[1:-1]
                for tr in tr_list:
                    detail_link = tr.select('td  a')[0].text
                    print(detail_link)
    
        if __name__ == '__main__':
            for i in range(0,100+1,10):
                t = MyThread(i)
                t.start()
      
    
    



# 存储问题


# json 文件写入是 转码 json.dumps(data,ensure_ascii=False)   写入json就是中文

# 爬取 数据存储mysql

## repr ()  函数 字符串原样输出 



# 读取文件 转码问题 在程序最上面加上：coding=gbk、

    import pymysql
    import requests
    import json
    # 连接数据库
    db = pymysql.connect('192.168.71.132','root','123456','xiaoshou',charset='utf8')
    
    cursor = db.cursor()
    
    # sql = "insert into hx VALUE (NULL,'{}','{}','{}')"#.format()
    sql = "insert into xh VALUE (NULL ,'孙洋','123123','321421','321421');"
    
    cursor.execute(sql) # 执行sql 语句
    db.commit()   #  提交事务
    # 查询中使用 fetchall  全部
    data = cursor.fetchone()  # 使用 fetchone() 方法获取单条数据.
     
    # print(data)
    db.close()
# sql  重复爬取 重复数据处理
     # sql = "insert into xh(sname,text,num,img) VALUES(%s,%s,%s,%s) on duplicate key update sname=values(sname)"
     注解： 从新爬取数据时 重复数据 更新  不会重复添加一个爬取过的数据

# 数据存储mysql
        import pymysql
        import datetime
    
    
        class Mysql():
            def __init__(self):
                try:
                    self.conn = pymysql.connect('192.168.71.132','root','123456','xiaoshou',charset='utf8')
                    self.cursor = self.conn.cursor()
                except Exception as e:
                    print(e)
            def mydb(self,sql,data):
                #  传入sql 语句 和 参数 data是一个（元组）
                try:
                    res = self.cursor.execute(sql,data)   
                    self.conn.commit()
                except Exception as e:
                    self.conn.rollback()
                    print(e)
                if __name__ == '__main__':
                    Mysql = Mysql()
                    # 系统当前时间
                    time_new1 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    print(time_new1)
                    # sql = "insert into xh(sname,text,num,img) VALUES(%s,%s,%s,%s) on duplicate key update"
                    # data = ['sunyang','123','123','432']
                    # Mysql.mydb(sql,data)
# to_csv 数据下载

 DataFrame_2.to_csv   ('Desktop\DataFrame_3.csv') 


# scrapy 爬虫框架



    #CONCURRENT_REQUESTS = 32  迸发数量

## scrapy 框架的安装
    1.
        如果已经安装好了 Anaconda ，那么可以通过 conda 命令安装 Scrapy 具体如下：
        conda install Scrapy 


    在你需要创建的文件里 运行
    
    创建 项目 scrapy startproject 项目名称
        cd  进入项目目录   
    生成一个爬虫文件
    
        scrapy genspider 爬虫名称 www.baidu.com
    
    运行一个爬虫
        scrapy crawl 爬虫名称
    
    
    生成爬虫文件后   和 爬虫项目一个目录
    from scrapy import cmdline
    # 创建入口文件
        from scrapy.cmdline import execute
        execute("scrapy crawl bole".split())

# scrapy print(repr(文本))  数据原样输出


# 用 selenium 抓取瀑布流  

    from selenium import webdriver
    
    browser = webdriver.Chrome(r'C:\Users\Administrator\AppData\Local\Google\Chrome\Application\chromedriver.exe')
    
    在里可以执行  JS 代
    获取 瀑布流 下一页
    browser.execure_script('scrollTo(0,decument,body.scrollHeight)')
    下一页  和 ajxj 传参的下一页一样
    请求对象.execure_script('js代码')


# 数据加载的方法
  前端完成JS 加载
  后端发送AJXJ 


# uuid.uuid4() 生成一个随机的 数字  
    2fe69fb2-8f93-4a84-9793-e8718ae46ef4
    <class 'uuid.UUID'>

# 时间运算 datetime import  timedelta 
        import datetime
        from datetime import  timedelta
        b = timedelta(days=3)
        a = datetime.datetime.now()
        print((a-b).strftime('%Y-%m-%d') )
        输出：2018-08-04

# 分布式爬虫 
    redis 
            编辑配置文件redis.conf
        1. 在服务器上下载redis 
            在redis.conf 文件中修改  注视
            # Examples:
            #
            # bind 192.168.1.100 10.0.0.1
            # bind 127.0.0.1 ::1
        2.  存储项  注释
            #   save ""
    
            # save 900 1
            # save 300 10
            # save 60 10000

# 分布式爬虫
        1.redis 服务
    
        2.确保scrapy-redis 环境已经安装
    
        3.确定单机scrapy 能够正常运行
    
        4.升级为分布式 settings：
    
        # url指纹过滤器
        DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
    
        # 调度器
        SCHEDULER = "scrapy_redis.scheduler.Scheduler"
        # 设置爬虫是否可以中断
        SCHEDULER_PERSIST = True
    
        # 设置请求队列类型
        # SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderPriorityQueue" # 按优先级入队列
        SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderQueue" # 按照队列模式
        # SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderStack" # 按照栈进行请求的调度
    
        # 配置redis管道文件，权重数字相对最大
        ITEM_PIPELINES = {
            'scrapy_redis.pipelines.RedisPipeline': 999, # redis管道文件，自动把数据加载到redis
        }
    
        redis 连接配置
        REDIS_HOST = '172.16.238.160'
        REDIS_PORT = 6379
    spider文件
    
    1.from scrapy_redis.spiders import RedisSpider，爬虫类需要继承 RedisSpider
    
    2.from scrapy_redis.spiders import RedisCrawlSpider  爬虫集成 RedisCrawlSpider
    
    设置爬虫的rediskey
    redis_key = '爬虫类名:start_urls'
    启动多台机器运行分布式爬虫程序：
    
    scrapy runspider 爬虫文件名：
    
    例如：cmdline.execute('scrapy runspider mycrawler_redis.py'.split())
    让爬虫工作起来
    
    连接redis ，添加一个列表
    
    lpush mycrawler:start_urls http://www.itxdl.cn(第一个抓取的url地址)

# redis 远程文件设置
    supervised no
    
    #   save ""   228行
    
    save 900 1
    save 300 10
    save 60 10000
	

	
# [selenium文档](https://germey.gitbooks.io/python3webspider/content/7.1-Selenium%E7%9A%84%E4%BD%BF%E7%94%A8.html) 
https://germey.gitbooks.io/python3webspider/content/7.1-Selenium%E7%9A%84%E4%BD%BF%E7%94%A8.html


# selenium  模拟浏览器 登录
    1. 获取 html 的所有内容
        from selenium import webdriver

        url = 'https://passport.baidu.com/v2/?login'

        driver = webdriver.Chrome()
        
        html = driver.page_source  # 显示网页的内容

        with open('ww.html','wb') as f:
            f.write(driver.page_source.encode('utf-8'
    2. 模拟登录后 获得所有的cookie 内容
        用模拟实力化的 对象 浏览器   
        driver = webdriver.Chrome()
        driver.get_cookie()

        得到的 cookie 是一个  多个字典 外面套一个列表的  数据
        例如：[{bid:arwGMINsy},{},{}]
        html  里的cookie 是一个 bid = arwGMINsy;
        获得的cookie 是一个 需要还遍历组合的
        遍历每一个字典 取出 name 和 value 的值  重新组合放在一个列表里  成为需要用的cookie  最后把列表 以 ；号 分隔  转换为str
        例如：
            for cookie_dict in cookie:
                cookie_str = cookie_dict['name'] + ''='' + cookie_dict['value']
                cookie_list.append(cookie_str)
            header_cookie = ';'.join(cookie_list)
        组合 需要的 headers 头 里的 cookie
        headers = {
                "cookie" : header_cookie
                "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36"
        }

        到下面 resquerst 里使用

# selenium  模拟浏览器 登录   打码 




[Selenium 之订制启动Chrome的选项（Options）](https://blog.csdn.net/duzilonglove/article/details/78517429)
https://blog.csdn.net/duzilonglove/article/details/78517429
# 1 try:  到 finally:
    “如果try中没有异常，那么except部分将跳过，执行else中的语句。（前提是try里没有返回值）

    finally是无论是否有异常，最后都要做的一些事情。”（无论try里是否有返回值）

    这里补充一句，在含有return的情况下，并不会阻碍finally的执行。（但是会阻碍else）
    new_url = request.urljoin(url,href)